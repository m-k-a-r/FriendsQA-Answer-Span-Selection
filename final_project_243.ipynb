{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_DTHEjvgFgz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe818c7f-cce1-499d-a733-38c027bb4a42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8 MB 25.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[K     |████████████████████████████████| 182 kB 101.2 MB/s \n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 68.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.9.24)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.11.0-py3-none-any.whl (512 kB)\n",
            "\u001b[K     |████████████████████████████████| 512 kB 33.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (4.1.1)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (1.12.1+cu113)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->torchmetrics) (3.0.9)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.11.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting seqeval\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[K     |████████████████████████████████| 43 kB 486 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.8/dist-packages (from seqeval) (1.21.6)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.8/dist-packages (from seqeval) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.7.3)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.2.0)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16179 sha256=36284d74944157c6b2aa1aec94adb1a24f3a9086d51eb49f39c51f25a73a8de5\n",
            "  Stored in directory: /root/.cache/pip/wheels/ad/5c/ba/05fa33fa5855777b7d686e843ec07452f22a66a138e290e732\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.3.0-py3-none-any.whl (72 kB)\n",
            "\u001b[K     |████████████████████████████████| 72 kB 1.8 MB/s \n",
            "\u001b[?25hCollecting xxhash\n",
            "  Downloading xxhash-3.1.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 73.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from evaluate) (21.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from evaluate) (1.3.5)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from evaluate) (4.64.1)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from evaluate) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.11.1)\n",
            "Collecting datasets>=2.0.0\n",
            "  Downloading datasets-2.7.1-py3-none-any.whl (451 kB)\n",
            "\u001b[K     |████████████████████████████████| 451 kB 75.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2022.11.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from evaluate) (0.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from evaluate) (2.23.0)\n",
            "Collecting multiprocess\n",
            "  Downloading multiprocess-0.70.14-py38-none-any.whl (132 kB)\n",
            "\u001b[K     |████████████████████████████████| 132 kB 90.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (3.8.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets>=2.0.0->evaluate) (9.0.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.8.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (22.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->evaluate) (3.0.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (2022.9.24)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->evaluate) (1.24.3)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 97.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->evaluate) (2022.6)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.15.0)\n",
            "Installing collected packages: urllib3, xxhash, responses, multiprocess, datasets, evaluate\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "Successfully installed datasets-2.7.1 evaluate-0.3.0 multiprocess-0.70.14 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install torchmetrics\n",
        "!pip install seqeval\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZwiTSVyLIlJ"
      },
      "source": [
        "# **Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGtzyNE63Cz6"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import transformers\n",
        "from transformers import BertTokenizerFast\n",
        "from transformers import DistilBertTokenizer, DistilBertForQuestionAnswering\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from torch.optim import AdamW\n",
        "import numpy as np\n",
        "from transformers import get_scheduler\n",
        "import evaluate\n",
        "from evaluate import load\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import BertForQuestionAnswering\n",
        "from transformers import BertTokenizer\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from datetime import datetime\n",
        "import collections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZOMyJls4Tdu"
      },
      "outputs": [],
      "source": [
        "# Load in json file and organize data into two dataframes for inputs and targets respectively\n",
        "def get_data(file):\n",
        "  f = open(file)\n",
        "  data = json.load(f)\n",
        "  x_df = pd.DataFrame(columns = ['context','question'])\n",
        "  y_df = pd.DataFrame(columns = ['start_pos','end_pos'])\n",
        "  for dat in data.get('data'):\n",
        "    for par in dat.get('paragraphs'):\n",
        "      # get context\n",
        "      context = \"\"\n",
        "      for utts in par.get('utterances:'):\n",
        "        # save string of format: \"<SPEAKER> speaker <SPEAKER> words words words <SPEAKER> speaker <SPEAKER> words words ...\" etc\n",
        "        speakers = \" \".join(utts.get('speakers'))\n",
        "        if speakers == '#NOTE#':\n",
        "          speakers = '<NOTE>'\n",
        "        utterance = utts.get('utterance')\n",
        "        #if necessary can remove speaker entirely\n",
        "        joined = '<SPEAKER> ' + speakers + ' <SPEAKER> ' + utterance\n",
        "        context = context + joined\n",
        "\n",
        "      # get question and answers corresponding to this context\n",
        "      for qas in par.get('qas'):\n",
        "        question = qas.get('question')\n",
        "        # add input to this dataframe\n",
        "        x_df.loc[len(x_df)] = [context, question]\n",
        "\n",
        "        # use first possible answer as target for this question\n",
        "        answer = qas.get('answers')[0]\n",
        "        text = answer.get('answer_text')\n",
        "        s_ind = context.index(text)\n",
        "        e_ind = s_ind + len(text)\n",
        "\n",
        "        # add target tuple to dataframe\n",
        "        y_df.loc[len(y_df)] = [torch.tensor(s_ind),torch.tensor(e_ind)]\n",
        "  return x_df,y_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQ0t4IoMeMUa"
      },
      "outputs": [],
      "source": [
        "# Make dataframes for each dataset split\n",
        "x_train,y_train = get_data('friendsqa_trn.json')\n",
        "x_valid,y_valid = get_data('friendsqa_dev.json')\n",
        "x_test,y_test = get_data('friendsqa_tst.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pPZ_S9Yv8sKS"
      },
      "source": [
        "# Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNKft6ZR9HWV"
      },
      "source": [
        "Plot is_speaker count distributions for each dataset split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANuscGNOEOL_"
      },
      "outputs": [],
      "source": [
        "# Plot counts of train targets that are marked as speakers\n",
        "trn_is_speaker_counts = y_train['is_speaker'].value_counts()\n",
        "print(trn_is_speaker_counts)\n",
        "plt.figure(figsize=(4, 4))\n",
        "ax = trn_is_speaker_counts.plot(kind=\"bar\")\n",
        "ax.set_xlabel('is_speaker')\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.set_title('Training Targets is_speaker Frequency')\n",
        "ax.set_xticklabels(labels=['False','True'])\n",
        "plt.xticks(rotation = 0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x33zQSQXJVGd"
      },
      "outputs": [],
      "source": [
        "# Plot counts of validation targets that are marked as speakers\n",
        "valid_is_speaker_counts = y_valid['is_speaker'].value_counts()\n",
        "print(valid_is_speaker_counts)\n",
        "plt.figure(figsize=(4, 4))\n",
        "ax = valid_is_speaker_counts.plot(kind=\"bar\")\n",
        "ax.set_xlabel('is_speaker')\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.set_title('Validation Targets is_speaker Frequency')\n",
        "ax.set_xticklabels(labels=['False','True'])\n",
        "plt.xticks(rotation = 0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1Ss94I8JViY"
      },
      "outputs": [],
      "source": [
        "# Plot counts of test targets that are marked as speakers\n",
        "tst_is_speaker_counts = y_test['is_speaker'].value_counts()\n",
        "print(tst_is_speaker_counts)\n",
        "\n",
        "plt.figure(figsize=(4, 4))\n",
        "ax = tst_is_speaker_counts.plot(kind=\"bar\")\n",
        "ax.set_xlabel('is_speaker')\n",
        "ax.set_ylabel(\"Frequency\")\n",
        "ax.set_title('Test Targets is_speaker Frequency')\n",
        "ax.set_xticklabels(labels=['False','True'])\n",
        "plt.xticks(rotation = 0)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fktWslVh9Bvq"
      },
      "source": [
        "Plot context length distributions for each dataset split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZajFJEpQ6OYi"
      },
      "outputs": [],
      "source": [
        "# Get context lengths for training set\n",
        "trn_context_lengths = x_train['context'].str.split().str.len().value_counts()\n",
        "trn_context_lengths = trn_context_lengths.reset_index()\n",
        "trn_context_lengths = trn_context_lengths.rename(columns={'index': 'length','text':'frequency'})\n",
        "\n",
        "# Print length (number of words) of shortest context\n",
        "print(\"Shortest context len: {}\".format(min(trn_context_lengths['length'])))\n",
        "# Print average length of context\n",
        "print(\"Mean context len: {}\".format(sum(trn_context_lengths['length'])/len(trn_context_lengths['length'])))\n",
        "# Print length of longest context\n",
        "print(\"Longest context len: {}\".format(max(trn_context_lengths['length'])))\n",
        "\n",
        "# Plot context length distribution for training set\n",
        "plt.hist(x_train['context'].str.split().str.len(),bins = 20)\n",
        "plt.xlabel(\"Context Lengths\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title('Training Context Length Frequencies')\n",
        "plt.figure(figsize=(3, 9))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6vs5JO_78ZnH"
      },
      "outputs": [],
      "source": [
        "# Get context lengths for validation set\n",
        "valid_context_lengths = x_valid['context'].str.split().str.len().value_counts()\n",
        "valid_context_lengths = valid_context_lengths.reset_index()\n",
        "valid_context_lengths = valid_context_lengths.rename(columns={'index': 'length','text':'frequency'})\n",
        "\n",
        "# Print length (number of words) of shortest context\n",
        "print(\"Shortest context len: {}\".format(min(valid_context_lengths['length'])))\n",
        "# Print average length of context\n",
        "print(\"Mean context len: {}\".format(sum(valid_context_lengths['length'])/len(valid_context_lengths['length'])))\n",
        "# Print length of longest context\n",
        "print(\"Longest context len: {}\".format(max(valid_context_lengths['length'])))\n",
        "\n",
        "# Plot context length distribution for validation set\n",
        "plt.hist(x_valid['context'].str.split().str.len(),bins = 20)\n",
        "plt.xlabel(\"Context Lengths\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title('Validation Context Length Frequencies')\n",
        "plt.figure(figsize=(3, 9))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TnX_Tra8aSB"
      },
      "outputs": [],
      "source": [
        "# Get context lengths for test set\n",
        "tst_context_lengths = x_test['context'].str.split().str.len().value_counts()\n",
        "tst_context_lengths = tst_context_lengths.reset_index()\n",
        "tst_context_lengths = tst_context_lengths.rename(columns={'index': 'length','text':'frequency'})\n",
        "\n",
        "# Print length (number of words) of shortest context\n",
        "print(\"Shortest context len: {}\".format(min(tst_context_lengths['length'])))\n",
        "# Print average length of context\n",
        "print(\"Mean context len: {}\".format(sum(tst_context_lengths['length'])/len(tst_context_lengths['length'])))\n",
        "# Print length of longest context\n",
        "print(\"Longest context len: {}\".format(max(tst_context_lengths['length'])))\n",
        "\n",
        "# Plot context length distribution for test set\n",
        "plt.hist(x_test['context'].str.split().str.len(),bins = 20)\n",
        "plt.xlabel(\"Context Lengths\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title('Test Context Length Frequencies')\n",
        "plt.figure(figsize=(3, 9))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzEoTCKq9ZOf"
      },
      "source": [
        "Explore the range of context lengths in the SQuAD 1.1 training data to compare against the FriendsQA data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5-XDO1wWQ4l"
      },
      "outputs": [],
      "source": [
        "# Make dataframe of SQuAD training data\n",
        "f = open('train-v1.1.json')\n",
        "data = json.load(f)\n",
        "squad_contexts = pd.DataFrame(columns = ['context'])\n",
        "count = 0\n",
        "for dat in data.get('data'):\n",
        "  for par in dat.get('paragraphs'):\n",
        "    context = par.get('context')\n",
        "    squad_contexts.loc[len(squad_contexts)] = [context]\n",
        "    count += 1\n",
        "\n",
        "# Get context lengths\n",
        "squad_context_lengths = squad_contexts['context'].str.split().str.len().value_counts()\n",
        "squad_context_lengths = squad_context_lengths.reset_index()\n",
        "squad_context_lengths = squad_context_lengths.rename(columns={'index': 'length','text':'frequency'})\n",
        "\n",
        "# Print length (number of words) of shortest context\n",
        "print(\"Shortest context len: {}\".format(min(squad_context_lengths['length'])))\n",
        "# Print average length of context\n",
        "print(\"Mean context len: {}\".format(sum(squad_context_lengths['length'])/len(squad_context_lengths['length'])))\n",
        "# Print length of longest context\n",
        "print(\"Longest context len: {}\".format(max(squad_context_lengths['length'])))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# BERT"
      ],
      "metadata": {
        "id": "dfwiqLFdt9Db"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BERTBASEQA(nn.Module):\n",
        "\n",
        "  def __init__(self, bert_type, hidden_size, num_labels):\n",
        "    super(BERTBASEQA, self).__init__()\n",
        "    self.bert_type = bert_type\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_labels = num_labels\n",
        "    self.bert = transformers.BertModel.from_pretrained(self.bert_type)\n",
        "    self.qa_outputs = nn.Linear(self.hidden_size, self.num_labels)\n",
        "\n",
        "  def forward(self, ids, token_ids):\n",
        "\n",
        "    output = self.bert(\n",
        "                      input_ids = ids, \n",
        "                      token_type_ids = token_ids\n",
        "                      )\n",
        "    \n",
        "    sequence_output = output[0]   #(None, seq_len, hidden_size)\n",
        "    logits = self.qa_outputs(sequence_output) #(None, seq_len, hidden_size)*(hidden_size, 2)=(None, seq_len, 2)\n",
        "    start_logits, end_logits = logits.split(1, dim=-1)    #(None, seq_len, 1), (None, seq_len, 1)\n",
        "    start_logits = start_logits.squeeze(-1)  #(None, seq_len)\n",
        "    end_logits = end_logits.squeeze(-1)    #(None, seq_len)\n",
        "\n",
        "\n",
        "    outputs = (start_logits, end_logits,) \n",
        "    \n",
        "    return outputs  "
      ],
      "metadata": {
        "id": "v8nHRmuMuApo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_func(out, s_target, e_target):\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  s_loss = criterion(out[0], s_target)\n",
        "  e_loss = criterion(out[1], e_target)\n",
        "  total_loss = s_loss+e_loss\n",
        "  return total_loss"
      ],
      "metadata": {
        "id": "X_Q7KzJTuIUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataloader"
      ],
      "metadata": {
        "id": "L3aB6sgxuWgG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertDataset(Dataset):\n",
        "  def __init__(self, tokenizer, context, question, max_length, text):\n",
        "    self.context = context\n",
        "    self.question = question\n",
        "    self.text = text\n",
        "    self.tokenizer = tokenizer\n",
        "    self.max_length = max_length\n",
        "    \n",
        "  def __len__(self):\n",
        "        return len(self.context)\n",
        "  \n",
        "  def __getitem__(self, idx):\n",
        "    context_ = self.context[idx]\n",
        "    question_ = self.question[idx]\n",
        "    text_ = self.text[idx]\n",
        "    \n",
        "    #encoding\n",
        "    input_ids = self.tokenizer.encode(question_, context_,padding=True,truncation=True,max_length=500, add_special_tokens = True)\n",
        "    answer_ids = self.tokenizer.encode(text_,padding=True,truncation=True,max_length=500, add_special_tokens = True)\n",
        "    token_type_ids = [0 if i <= input_ids.index(102) else 1 for i in range(len(input_ids))]\n",
        "    \n",
        "    #calculating start and end position of answer in input_ids\n",
        "    s_pos, e_pos = 0, 0\n",
        "    for i in range(len(input_ids)):\n",
        "      if (input_ids[i: i+len(answer_ids[1:-1])] == answer_ids[1:-1]):\n",
        "        s_pos = i\n",
        "        e_pos = i + len(answer_ids[1:-1]) - 1\n",
        "        break\n",
        "\n",
        "    assert((s_pos<len(input_ids)) & (e_pos<len(input_ids)) & (s_pos<=e_pos))\n",
        "    \n",
        "    if (len(input_ids)<self.max_length):\n",
        "      padding_len = self.max_length - len(input_ids)\n",
        "      ids = input_ids + ([0]*padding_len)\n",
        "    else:\n",
        "      ids = input_ids[:self.max_length]\n",
        "\n",
        "    if (len(token_type_ids)<self.max_length):\n",
        "      padding_len = self.max_length - len(token_type_ids)\n",
        "      token_ids = token_type_ids  + ([1]*padding_len)\n",
        "    else:\n",
        "      token_ids = token_type_ids[:self.max_length]\n",
        " \n",
        "    return {'ids': torch.tensor(ids, dtype = torch.long),\n",
        "            'token_type_ids': torch.tensor(token_ids, dtype = torch.long),\n",
        "            'start_pos': torch.tensor(s_pos, dtype = torch.long),\n",
        "            'end_pos': torch.tensor(e_pos, dtype = torch.long)}          "
      ],
      "metadata": {
        "id": "L_wfFeIduXVO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training BERT Baseline Model"
      ],
      "metadata": {
        "id": "0TPeqtQ4uNM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(dataloader, model, optimizer, device, max_grad_norm, scheduler=None):\n",
        "  model.train()\n",
        "  for bi, d in enumerate(notebook.tqdm(dataloader, desc=\"Iteration\")):\n",
        "    ids = d['ids']\n",
        "    # mask_ids = d['mask']\n",
        "    token_ids = d['token_type_ids']\n",
        "    start_pos = d['start_pos']\n",
        "    end_pos = d['end_pos']\n",
        "\n",
        "    ids = ids.to(device, dtype = torch.long)\n",
        "    # mask_ids = mask_ids.to(device, dtype = torch.long)\n",
        "    token_ids = token_ids.to(device, dtype = torch.long)\n",
        "    start_pos = start_pos.to(device, dtype = torch.long)\n",
        "    end_pos = end_pos.to(device, dtype = torch.long)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    start_and_end_scores = model(ids, token_ids)\n",
        "    # start_scores, end_scores = model(ids, token_ids)\n",
        "    loss = loss_func(start_and_end_scores, start_pos, end_pos)\n",
        "    # torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if scheduler is not None:\n",
        "      scheduler.step()\n",
        "    if bi%100==0:\n",
        "      print (f\"bi: {bi}, loss: {loss}\")\n",
        "    all_train_loss.append(loss)"
      ],
      "metadata": {
        "id": "db-az2aWuPCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def eval(dataloader, model, device):\n",
        "  model.eval()\n",
        "  pred_s = None\n",
        "  pred_e = None\n",
        "  eval_loss = 0.0\n",
        "  eval_steps = 0\n",
        "\n",
        "  for bi, d in enumerate(dataloader):\n",
        "    ids = d['ids']\n",
        "    # mask_ids = d['mask']\n",
        "    token_ids = d['token_type_ids']\n",
        "    start_pos = d['start_pos']\n",
        "    end_pos = d['end_pos']\n",
        "\n",
        "    ids = ids.to(device, dtype = torch.long)\n",
        "    # mask_ids = mask_ids.to(device, dtype = torch.long)\n",
        "    token_ids = token_ids.to(device, dtype = torch.long)\n",
        "    start_pos = start_pos.to(device, dtype = torch.long)\n",
        "    end_pos = end_pos.to(device, dtype = torch.long)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      start_and_end_scores = model(ids, token_ids)\n",
        "      loss = loss_func(start_and_end_scores, start_pos, end_pos)\n",
        "      eval_loss += loss.mean().item()\n",
        "    \n",
        "    eval_steps += 1\n",
        "    if pred_s is None:\n",
        "      pred_s = start_and_end_scores[0].detach().cpu().numpy()\n",
        "      pred_e = start_and_end_scores[1].detach().cpu().numpy()\n",
        "    else:\n",
        "      pred_s = np.append(pred_s, start_and_end_scores[0].detach().cpu().numpy(), axis=0)\n",
        "      pred_e = np.append(pred_e, start_and_end_scores[1].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "  eval_loss = eval_loss/eval_steps\n",
        "  pred_start = np.argmax(pred_s, axis=1)\n",
        "  pred_end = np.argmax(pred_e, axis=1)\n",
        "  all_val_loss.append(eval_loss)\n",
        "  return eval_loss, pred_start, pred_end"
      ],
      "metadata": {
        "id": "f4QrWq5xufiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_SEQ_LENGTH = 512\n",
        "TRAIN_BATCH_SIZE = 8\n",
        "EVAL_BATCH_SIZE = 8\n",
        "LEARNING_RATE = 1e-5\n",
        "NUM_TRAIN_EPOCHS = 2\n",
        "BERT_TYPE = \"bert-base-uncased\"\n",
        "max_grad_norm = 1.0"
      ],
      "metadata": {
        "id": "evaBEFNEugGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = transformers.BertTokenizer.from_pretrained(BERT_TYPE)\n",
        "train_dataset = BertDataset(\n",
        "    tokenizer = tokenizer,\n",
        "    context = train_data['context'],\n",
        "    question = train_data['question'],\n",
        "    max_length = MAX_SEQ_LENGTH,\n",
        "    text = train_data['text']\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size = TRAIN_BATCH_SIZE, shuffle=True)"
      ],
      "metadata": {
        "id": "MD1m-N22uiR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "eval_dataset = BertDataset(\n",
        "    tokenizer = tokenizer,\n",
        "    context = valid_data['context'],\n",
        "    question = valid_data['question'],\n",
        "    max_length = MAX_SEQ_LENGTH,\n",
        "    text = valid_data['text']\n",
        ") \n",
        "\n",
        "eval_dataloader = DataLoader(eval_dataset, batch_size = EVAL_BATCH_SIZE, shuffle=False)"
      ],
      "metadata": {
        "id": "GeMNPHIjukWR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)\n",
        "\n",
        "model = BERTBASEQA(BERT_TYPE, 768, 2).to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, correct_bias=False)\n",
        "\n",
        "NUM_TRAIN_STEPS = int(len(train_dataset)/TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS) \n",
        "scheduler = transformers.get_constant_schedule_with_warmup(\n",
        "                optimizer, \n",
        "                num_warmup_steps=500,\n",
        "                # num_training_steps=NUM_TRAIN_STEPS,\n",
        "                last_epoch=-1)"
      ],
      "metadata": {
        "id": "hLUxOmLGuomg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training\n",
        "for epoch in trange(NUM_TRAIN_EPOCHS):\n",
        "  train(train_dataloader, model, optimizer, device, max_grad_norm, scheduler)"
      ],
      "metadata": {
        "id": "LMr1cxU_upiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Model"
      ],
      "metadata": {
        "id": "zuznHkX4utDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "res = eval(eval_dataloader, model, device)\n",
        "print(res[0])"
      ],
      "metadata": {
        "id": "XPBgni26uuEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "context_ = valid_data['context']\n",
        "question_ = valid_data['question']\n",
        "text_ = valid_data['text']\n",
        "pred_start = res[1]\n",
        "pred_end = res[2]\n",
        "res_text_ = []\n",
        "act_start = []\n",
        "act_end = []\n",
        "\n",
        "\n",
        "input_ids_list = list(map(lambda x,y: tokenizer.encode(x, y, padding=True,truncation=True,max_length=500, add_special_tokens = True), question_, context_))\n",
        "answer_ids_list = list(map(lambda x: tokenizer.encode(x,padding=True,truncation=True,max_length=500, add_special_tokens = True), text_))\n",
        "\n",
        "for i in range(len(input_ids_list)):\n",
        "  res_text_.append(tokenizer.decode(input_ids_list[i][pred_start[i]:pred_end[i]+1]))\n",
        "\n",
        "  s_pos, e_pos = 0, 0\n",
        "  for j in range(len(input_ids_list[i])):\n",
        "    if (input_ids_list[i][j: j+len(answer_ids_list[i][1:-1])] == answer_ids_list[i][1:-1]):\n",
        "      s_pos = j\n",
        "      e_pos = j + len(answer_ids_list[i][1:-1]) - 1\n",
        "      break\n",
        "  act_start.append(s_pos)\n",
        "  act_end.append(e_pos)"
      ],
      "metadata": {
        "id": "NR19Wkequv1z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_data['start_pos'] = act_start\n",
        "valid_data['end_pos'] = act_end\n",
        "valid_data['predicted_text'] = res_text_\n",
        "valid_data['predicted_start_pos'] = pred_start\n",
        "valid_data['predicted_end_pos'] = pred_end"
      ],
      "metadata": {
        "id": "AiFnAyNMu0O-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_columns = ['text', 'predicted_text', 'start_pos', 'end_pos', 'predicted_start_pos', 'predicted_end_pos']\n",
        "valid_data[show_columns].head(20)"
      ],
      "metadata": {
        "id": "5i5iWjq-u2QG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculates incorrect data based on start and end positions\n",
        "cond1 = valid_data['predicted_start_pos']>valid_data['predicted_end_pos']\n",
        "cond2 = valid_data['end_pos']<valid_data['predicted_start_pos']\n",
        "cond3 = valid_data['start_pos']>valid_data['predicted_end_pos']\n",
        "\n",
        "incorrect_pred = valid_data[(cond1) | (cond2) | (cond3)].shape[0]\n",
        "incorrect_pred"
      ],
      "metadata": {
        "id": "Ffvt-YHSvF4d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = valid_data.shape[0]\n",
        "print(f\"accuracy = {(t - incorrect_pred)*100/t}\")"
      ],
      "metadata": {
        "id": "QOB0Xu_uu8EP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3dnr7SxCLNvV"
      },
      "source": [
        "# Finetuning BERT for QA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X752NMjpsc1h"
      },
      "source": [
        "Format data for this model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j60-f_Fwp5V_"
      },
      "outputs": [],
      "source": [
        "# Dataset class\n",
        "class BERTDataset(Dataset):\n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "\n",
        "  def __getitem__(self,i):\n",
        "    input = self.data[i][0]\n",
        "    attention = self.data[i][1]\n",
        "    start = self.data[i][2]\n",
        "    end = self.data[i][3]\n",
        "    return [input,attention,start,end]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "id": "96R3Kuszra4U",
        "outputId": "4d7aa456-e2d1-4f3e-9be6-8546a9c14457"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-aac0f7f6eb79>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Encode data using the BERT tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertTokenizerFast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert-base-uncased'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtrain_encodings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion_trn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *init_inputs, **kwargs)\u001b[0m\n\u001b[1;32m   1722\u001b[0m                 \u001b[0;31m# Try to get the tokenizer config to see if there are versioned tokenizer files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m                 \u001b[0mfast_tokenizer_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFULL_TOKENIZER_FILE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1724\u001b[0;31m                 resolved_config_file = cached_file(\n\u001b[0m\u001b[1;32m   1725\u001b[0m                     \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1726\u001b[0m                     \u001b[0mTOKENIZER_CONFIG_FILE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/transformers/utils/hub.py\u001b[0m in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, subfolder, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash)\u001b[0m\n\u001b[1;32m    407\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# Load from URL or cache if already cached\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         resolved_file = hf_hub_download(\n\u001b[0m\u001b[1;32m    410\u001b[0m             \u001b[0mpath_or_repo_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m             )\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"downloading %s to %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1242\u001b[0;31m             http_get(\n\u001b[0m\u001b[1;32m   1243\u001b[0m                 \u001b[0murl_to_download\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m                 \u001b[0mtemp_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36mhttp_get\u001b[0;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresume_size\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Range\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"bytes=%d-\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mresume_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m     r = _request_wrapper(\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/file_download.py\u001b[0m in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, timeout, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;31m# 3. Exponential backoff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m     return http_backoff(\n\u001b[0m\u001b[1;32m    439\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhttp_backoff\u001b[0;34m(method, url, max_retries, base_wait_time, max_wait_time, retry_on_exceptions, retry_on_status_codes, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0;31m# Perform request and return if status_code is not in the retry list.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mretry_on_status_codes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    528\u001b[0m         }\n\u001b[1;32m    529\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m                 resp = conn.urlopen(\n\u001b[0m\u001b[1;32m    440\u001b[0m                     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m                     \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m             \u001b[0;31m# Make the request on the httplib connection object.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 670\u001b[0;31m             httplib_response = self._make_request(\n\u001b[0m\u001b[1;32m    671\u001b[0m                 \u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    379\u001b[0m         \u001b[0;31m# Trigger any extra validation we need to do.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 381\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    382\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m             \u001b[0;31m# Py2 raises this as a BaseSSLError, Py3 raises it as socket timeout.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    976\u001b[0m         \u001b[0;31m# Force connect early to allow us to validate the connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    977\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 978\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    979\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_verified\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    360\u001b[0m             \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_default_certs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m         self.sock = ssl_wrap_socket(\n\u001b[0m\u001b[1;32m    363\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0mkeyfile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/urllib3/util/ssl_.py\u001b[0m in \u001b[0;36mssl_wrap_socket\u001b[0;34m(sock, keyfile, certfile, cert_reqs, ca_certs, server_hostname, ssl_version, ciphers, ssl_context, ca_cert_dir, key_password, ca_cert_data)\u001b[0m\n\u001b[1;32m    384\u001b[0m     ) or IS_SECURETRANSPORT:\n\u001b[1;32m    385\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mHAS_SNI\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mserver_hostname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrap_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mserver_hostname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_hostname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         warnings.warn(\n",
            "\u001b[0;32m/usr/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mwrap_socket\u001b[0;34m(self, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, session)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# SSLSocket class handles server_hostname encoding before it calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0;31m# ctx._wrap_socket()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         return self.sslsocket_class._create(\n\u001b[0m\u001b[1;32m    501\u001b[0m             \u001b[0msock\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    502\u001b[0m             \u001b[0mserver_side\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mserver_side\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36m_create\u001b[0;34m(cls, sock, server_side, do_handshake_on_connect, suppress_ragged_eofs, server_hostname, context, session)\u001b[0m\n\u001b[1;32m   1038\u001b[0m                         \u001b[0;31m# non-blocking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m                         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"do_handshake_on_connect should not be specified for non-blocking sockets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.8/ssl.py\u001b[0m in \u001b[0;36mdo_handshake\u001b[0;34m(self, block)\u001b[0m\n\u001b[1;32m   1307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1309\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_handshake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1310\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Get lists of contexts for each split\n",
        "context_trn = list(x_train['context'])\n",
        "context_valid = list(x_valid['context'])\n",
        "context_tst = list(x_test['context'])\n",
        "\n",
        "# Get lists of questions for each split\n",
        "question_trn = list(x_train['question'])\n",
        "question_valid = list(x_valid['question'])\n",
        "question_tst = list(x_test['question'])\n",
        "\n",
        "# Encode data using the BERT tokenizer\n",
        "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
        "\n",
        "train_encodings = tokenizer(context_trn, question_trn, truncation=True, padding=True)\n",
        "valid_encodings = tokenizer(context_valid, question_valid, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(context_tst, question_tst, truncation=True, padding=True)\n",
        "\n",
        "# Get lists of start positions\n",
        "train_start = list(y_train['start_pos'])\n",
        "valid_start = list(y_valid['start_pos'])\n",
        "test_start = list(y_test['start_pos'])\n",
        "\n",
        "# Get lists of end positions\n",
        "train_end = list(y_train['end_pos'])\n",
        "valid_end = list(y_valid['end_pos'])\n",
        "test_end = list(y_test['end_pos'])\n",
        "\n",
        "# Get data for training from tokenizer encodings\n",
        "train_inputs = train_encodings['input_ids']\n",
        "train_sents = train_encodings['token_type_ids']\n",
        "train_attention = train_encodings['attention_mask']\n",
        "train_tokens = [tokenizer.convert_ids_to_tokens(i) for i in train_inputs]\n",
        "\n",
        "# Get data for validation from tokenizer encodings\n",
        "valid_inputs = valid_encodings['input_ids']\n",
        "valid_sents = valid_encodings['token_type_ids']\n",
        "valid_attention = valid_encodings['attention_mask']\n",
        "valid_tokens = [tokenizer.convert_ids_to_tokens(i) for i in valid_inputs]\n",
        "\n",
        "# Get data for testing from tokenizer encodings\n",
        "test_inputs = test_encodings['input_ids']\n",
        "test_sents = test_encodings['token_type_ids']\n",
        "test_attention = test_encodings['attention_mask']\n",
        "test_tokens = [tokenizer.convert_ids_to_tokens(i) for i in test_inputs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CrrdycZEha2K"
      },
      "outputs": [],
      "source": [
        "# Make datasets for each split using one tenth of the data\n",
        "train_dataset = BERTDataset(list(zip(torch.tensor(train_inputs[0:int(len(train_inputs)/10)]),torch.tensor(train_attention[0:int(len(train_attention)/10)]),torch.stack(train_start[0:int(len(train_start)/10)]),torch.stack(train_end[0:int(len(train_end)/10)]))))\n",
        "valid_dataset = BERTDataset(list(zip(torch.tensor(valid_inputs[0:int(len(valid_inputs)/10)]),torch.tensor(valid_attention[0:int(len(valid_attention)/10)]),torch.stack(valid_start[0:int(len(valid_start)/10)]),torch.stack(valid_end[0:int(len(valid_end)/10)]))))\n",
        "test_dataset = BERTDataset(list(zip(torch.tensor(test_inputs[0:int(len(test_inputs)/10)]),torch.tensor(test_attention[0:int(len(test_attention)/10)]),torch.stack(test_start[0:int(len(test_start)/10)]),torch.stack(test_end[0:int(len(test_end)/10)]))))\n",
        "\n",
        "# Create dataloaders\n",
        "train_dataloader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "valid_dataloader = DataLoader(dataset=valid_dataset, batch_size=64, shuffle=False)\n",
        "test_dataloader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "almj8JcSr8Sr"
      },
      "source": [
        "Define model and training parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3TxQ5eqhayo"
      },
      "outputs": [],
      "source": [
        "# Model\n",
        "model = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
        "model_checkpoint = 'bert-base-uncased'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hK9JRFvNhaqQ"
      },
      "outputs": [],
      "source": [
        "# Init parameters for training\n",
        "optimizer = AdamW(model.parameters(), lr=2e-5)\n",
        "\n",
        "num_epochs = 10\n",
        "num_training_steps = num_epochs * len(train_dataloader)\n",
        "\n",
        "lr_scheduler = get_scheduler(\n",
        "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "device = torch.device(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "model = model.to(device)\n",
        "progress_bar = tqdm(range(num_training_steps))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pb6euGaQr4tF"
      },
      "source": [
        "Define functions for evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCGJCvoSr4LG"
      },
      "outputs": [],
      "source": [
        "# Calculate F1 Score\n",
        "def compute_f1(predictions, targets):\n",
        "  avg_f1 = 0\n",
        "  for i in range(len(predictions)):\n",
        "    f1 = 0\n",
        "\n",
        "    curr_pred = predictions[i]\n",
        "    curr_targ = targets[i]\n",
        "    shared_tokens = [x for x in curr_pred.split() if x in curr_targ.split()]\n",
        "\n",
        "    # if no tokens are the same then f1 = 0\n",
        "    if len(shared_tokens) == 0:\n",
        "        f1 = 0\n",
        "    else:\n",
        "      precision = len(shared_tokens) / len(curr_pred.split())\n",
        "      recall = len(shared_tokens) / len(curr_targ.split())\n",
        "      \n",
        "      f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    avg_f1 += f1\n",
        "  return avg_f1 / len(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n7oK5c4dsBee"
      },
      "outputs": [],
      "source": [
        "# Compute exact match, ignoring case\n",
        "def compute_em(predictions,targets):\n",
        "  em = 0\n",
        "  for i in range(len(predictions)):\n",
        "    curr_pred = predictions[i].lower()\n",
        "    curr_targ = targets[i].lower()\n",
        "\n",
        "    if curr_pred == curr_targ:\n",
        "      em += 1\n",
        "\n",
        "  return round(em / len(predictions), 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFg0l-bto0Tr"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZhyWJbYham6"
      },
      "outputs": [],
      "source": [
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    for batch in train_dataloader:\n",
        "        inputs = batch[0].to(device)\n",
        "        attention = batch[1].to(device)\n",
        "        start_positions = batch[2].to(device)\n",
        "        end_positions = batch[3].to(device)\n",
        "\n",
        "        outputs = model(inputs,attention_mask = attention, start_positions=start_positions, end_positions=end_positions)\n",
        "\n",
        "        # Compute loss, gradients and update model\n",
        "        loss = outputs.loss\n",
        "        print(\"loss: {}\".format(outputs.loss))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "        optimizer.zero_grad()\n",
        "        progress_bar.update(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqpK3f_2nqox"
      },
      "source": [
        "Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G07T-R4simX3"
      },
      "outputs": [],
      "source": [
        "# Init variables for keeping track of f1 and em during validation\n",
        "v_count = 0\n",
        "v_avg_f1 = 0\n",
        "v_avg_em = 0\n",
        "f1_val = []\n",
        "\n",
        "model.eval()\n",
        "for batch in valid_dataloader:\n",
        "    with torch.no_grad():\n",
        "        v_count += 1\n",
        "        inputs = batch[0].to(device)\n",
        "        attention = batch[1].to(device)\n",
        "        start_positions = batch[2].to(device)\n",
        "        end_positions = batch[3].to(device)\n",
        "\n",
        "        # Get model predictions\n",
        "        outputs = model(inputs,attention_mask = attention, start_positions=start_positions, end_positions=end_positions)\n",
        "\n",
        "        # Get predicted start and end indices from model outputs\n",
        "        start_logits = outputs.start_logits\n",
        "        start_preds = torch.argmax(start_logits,dim=1)\n",
        "        end_logits = outputs.end_logits\n",
        "        end_preds = torch.argmax(end_logits,dim=1)\n",
        "        \n",
        "        # Convert predicted indices to answer spans\n",
        "        predictions = []\n",
        "        for i in range(len(start_preds)):\n",
        "          answer_start = start_preds[i]\n",
        "          answer_end = end_preds[i]\n",
        "          answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[0][answer_start:answer_end]))\n",
        "          predictions.append(answer)\n",
        "\n",
        "        # Convert target indices to answer spans\n",
        "        targets = []\n",
        "        for i in range(len(start_positions)):\n",
        "          answer_start = start_positions[i]\n",
        "          answer_end = end_positions[i]\n",
        "          answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[0][answer_start:answer_end]))\n",
        "          targets.append(answer)\n",
        "\n",
        "        # Calculate f1\n",
        "        v_f1 = compute_f1(predictions, targets)\n",
        "        f1_val.append(v_f1)\n",
        "        v_avg_f1 += v_f1\n",
        "        print(\"F1 for this batch: {}\".format(v_f1))\n",
        "        print('f1 validation', f1_val)\n",
        "\n",
        "        # Calculate exact match\n",
        "        v_em = compute_em(predictions, targets)\n",
        "        print(\"EM for this batch: {}\".format(v_em))\n",
        "        v_avg_em += v_em\n",
        "\n",
        "        \"\"\"# count all exact match\n",
        "        for i in range(len(targets)):\n",
        "          if targets[i] == predictions[i]:\n",
        "            count_EM +=1\"\"\"\n",
        "\n",
        "# Print average F1 score and exact match\n",
        "print(\"Average Validation F1: {}\".format(v_avg_f1/v_count))\n",
        "print(\"Average Validation EM: {}\".format(v_avg_em/v_count))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh14w4aKoOr0"
      },
      "source": [
        "Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-zkzSZRbimOP"
      },
      "outputs": [],
      "source": [
        "# Init variables for keeping track of f1 and em during validation\n",
        "t_count = 0\n",
        "t_avg_f1 = 0\n",
        "t_avg_em = 0\n",
        "f1_test = []\n",
        "\n",
        "model.eval()\n",
        "for batch in test_dataloader:\n",
        "    with torch.no_grad():\n",
        "        t_count += 1\n",
        "        inputs = batch[0].to(device)\n",
        "        attention = batch[1].to(device)\n",
        "        start_positions = batch[2].to(device)\n",
        "        end_positions = batch[3].to(device)\n",
        "\n",
        "        # Get model predictions\n",
        "        outputs = model(inputs,attention_mask = attention, start_positions=start_positions, end_positions=end_positions)\n",
        "\n",
        "        start_logits = outputs.start_logits\n",
        "        start_preds = torch.argmax(start_logits,dim=-1)\n",
        "        end_logits = outputs.end_logits\n",
        "        end_preds = torch.argmax(end_logits,dim=-1)\n",
        "\n",
        "        # Convert predicted indices to answer spans\n",
        "        predictions = []\n",
        "        for i in range(len(start_preds)):\n",
        "          answer_start = start_preds[i]\n",
        "          answer_end = end_preds[i]\n",
        "          answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[0][answer_start:answer_end]))\n",
        "          predictions.append(answer)\n",
        "\n",
        "        # Convert target indices to answer spans\n",
        "        targets = []\n",
        "        for i in range(len(start_positions)):\n",
        "          answer_start = start_positions[i]\n",
        "          answer_end = end_positions[i]\n",
        "          answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(inputs[0][answer_start:answer_end]))\n",
        "          targets.append(answer)\n",
        "\n",
        "        # Calculate f1\n",
        "        t_f1 = compute_f1(predictions, targets)\n",
        "        f1_test.append(t_f1)\n",
        "        t_avg_f1 += t_f1\n",
        "        print(\"F1 for this batch: {}\".format(t_f1))\n",
        "        print('f1 test', f1_test)\n",
        "\n",
        "        # Calculate exact match\n",
        "        t_em = compute_em(predictions, targets)\n",
        "        print(\"EM for this batch: {}\".format(t_em))\n",
        "        t_avg_em += t_em\n",
        "\n",
        "# Print average F1 score and exact match\n",
        "print(\"Average Test F1: {}\".format(t_avg_f1/t_count))\n",
        "print(\"Average Test EM: {}\".format(t_avg_em/t_count))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfGrUXkJimBN"
      },
      "outputs": [],
      "source": [
        "# Write f1 scores from training and validation into a csv\n",
        "with open('some.csv', 'wb') as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerows(zip(f1_val, f1_test)) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6UrPfTBCFXPt"
      },
      "source": [
        "# Bi-LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZBuI2JgqM6k"
      },
      "source": [
        "Define Bi-LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1C-_y3lb0Gq"
      },
      "outputs": [],
      "source": [
        "# Simple 2-Layer BiLSTM\n",
        "class BiLSTM(nn.Module):\n",
        "  def __init__(self, n_tokens, emb_dim, h_dim, l_in, out_dim):\n",
        "    super().__init__()\n",
        "    self.embedding = nn.Embedding(n_tokens, emb_dim, padding_idx=0)\n",
        "    self.rnn = nn.LSTM(emb_dim,h_dim,bidirectional = True,batch_first=True,num_layers=2,dropout=0.5)\n",
        "    self.fc = nn.Linear(h_dim*2*l_in, out_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.embedding(x)\n",
        "    out = F.dropout(x,p=0.2)\n",
        "    out, hidden = self.rnn(x)\n",
        "    out = torch.flatten(out,start_dim=1,end_dim=2)\n",
        "    out = self.fc(out)\n",
        "    out = F.dropout(out,p=0.5)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzOASNqrqPOW"
      },
      "source": [
        "Format data for this model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wn-5gj00LhCN"
      },
      "outputs": [],
      "source": [
        "#Dataset class\n",
        "class LSTMDataset(Dataset):\n",
        "  #Take in raw data, convert to vectorized utterances and encoded labels\n",
        "  def __init__(self, data):\n",
        "    self.data = data\n",
        "\n",
        "  def __getitem__(self,i):\n",
        "    inputs = self.data[i][0]\n",
        "    labels = self.data[i][1]\n",
        "    return [inputs,labels]\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mqg-OIc2ULoK"
      },
      "outputs": [],
      "source": [
        "# Create inputs for bi-LSTM model (slightly different format than for BERT as we aren't using the BERT tokenizer)\n",
        "x_train['inputs'] = x_train['context'] + ' <sep> ' + x_train['question']\n",
        "x_valid['inputs'] = x_valid['context'] + ' <sep> ' + x_valid['question']\n",
        "x_test['inputs'] = x_test['context'] + ' <sep> ' + x_test['question']\n",
        "\n",
        "x_train_lst = list(x_train['inputs'])\n",
        "x_valid_lst = list(x_valid['inputs'])\n",
        "x_test_lst = list(x_test['inputs'])\n",
        "\n",
        "# Get lists of contexts for each split\n",
        "context_trn = list(x_train['context'])\n",
        "context_valid = list(x_valid['context'])\n",
        "context_tst = list(x_test['context'])\n",
        "\n",
        "# Get lists of start positions\n",
        "train_start = list(y_train['start_pos'])\n",
        "valid_start = list(y_valid['start_pos'])\n",
        "test_start = list(y_test['start_pos'])\n",
        "\n",
        "# Get lists of end positions\n",
        "train_end = list(y_train['end_pos'])\n",
        "valid_end = list(y_valid['end_pos'])\n",
        "test_end = list(y_test['end_pos'])\n",
        "\n",
        "# Use vectorizer to get vocab keys\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(x_train_lst)\n",
        "\n",
        "# Make vocab\n",
        "vocab = dict()\n",
        "vocab['none'] = 0\n",
        "i = 1\n",
        "for key in vectorizer.vocabulary_.keys():\n",
        "  vocab[key] = int(i)\n",
        "  i += 1\n",
        "vocab['<unk>'] = i\n",
        "vocab_size = len(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ca4kcRFYUklf"
      },
      "outputs": [],
      "source": [
        "# Return padded utterance with each word encoded as the correct index from the vocab\n",
        "# Default: <unk>\n",
        "def encode_tokens(x,vocab,max_utterance_len):\n",
        "  tokens = x.split()\n",
        "  encoded = torch.zeros(max_utterance_len)\n",
        "  count = 0\n",
        "  for i in range(len(tokens)):\n",
        "    # Truncate utterances longer than max length parameter\n",
        "    if count == max_utterance_len:\n",
        "      break\n",
        "    if tokens[i] in vocab.keys():\n",
        "        encoded[i] = vocab.get(tokens[i],vocab['<unk>'])\n",
        "    else:\n",
        "        encoded[i] = vocab.get('<unk>')\n",
        "    count += 1\n",
        "  return encoded.type(torch.LongTensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eTIVbpxGJH_C"
      },
      "outputs": [],
      "source": [
        "# Set max length of utterances to 1000\n",
        "max_len = 1500\n",
        "      \n",
        "# Encode input for bi-LSTM model\n",
        "encoded_train = [encode_tokens(x,vocab,max_len) for x in x_train_lst]\n",
        "encoded_valid = [encode_tokens(x,vocab,max_len) for x in x_valid_lst]\n",
        "encoded_test = [encode_tokens(x,vocab,max_len) for x in x_test_lst]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9k97lebHkIv"
      },
      "outputs": [],
      "source": [
        "# Convert start and end targets for each batch into tensor for training, validation and testing (using one tenth of the train data)\n",
        "train_targets = torch.stack((torch.stack(train_start),torch.stack(train_end)),dim=1)\n",
        "valid_targets = torch.stack((torch.stack(valid_start),torch.stack(valid_end)),dim=1)\n",
        "test_targets = torch.stack((torch.stack(test_start),torch.stack(test_end)),dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UMIWRVcbHIQ6"
      },
      "outputs": [],
      "source": [
        "# Make datasets for each split\n",
        "lstm_train_dataset = LSTMDataset(list(zip(encoded_train,train_targets)))\n",
        "lstm_valid_dataset = LSTMDataset(list(zip(encoded_valid,valid_targets)))\n",
        "lstm_test_dataset = LSTMDataset(list(zip(encoded_test,test_targets)))\n",
        "\n",
        "# Create dataloaders\n",
        "lstm_train_dataloader = DataLoader(dataset=lstm_train_dataset, batch_size=64, shuffle=True)\n",
        "lstm_valid_dataloader = DataLoader(dataset=lstm_valid_dataset, batch_size=64, shuffle=False)\n",
        "lstm_test_dataloader = DataLoader(dataset=lstm_test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Taui2m4xr0AM"
      },
      "source": [
        "Define functions for evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKuCCEaIrvZo"
      },
      "outputs": [],
      "source": [
        "# Calculate F1 Score\n",
        "def compute_f1(predictions, targets):\n",
        "  avg_f1 = 0\n",
        "  for i in range(len(predictions)):\n",
        "    f1 = 0\n",
        "\n",
        "    curr_pred = predictions[i]\n",
        "    curr_targ = targets[i]\n",
        "    shared_tokens = [x for x in curr_pred.split() if x in curr_targ.split()]\n",
        "\n",
        "    # if no tokens are the same then f1 = 0\n",
        "    if len(shared_tokens) == 0:\n",
        "        f1 = 0\n",
        "    else:\n",
        "      precision = len(shared_tokens) / len(curr_pred.split())\n",
        "      recall = len(shared_tokens) / len(curr_targ.split())\n",
        "      \n",
        "      f1 = 2 * (precision * recall) / (precision + recall)\n",
        "    avg_f1 += f1\n",
        "  return avg_f1 / len(predictions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4OxSzzGrysT"
      },
      "outputs": [],
      "source": [
        "# Compute exact match, ignoring case\n",
        "def compute_em(predictions,targets):\n",
        "  em = 0\n",
        "  for i in range(len(predictions)):\n",
        "    curr_pred = predictions[i].lower()\n",
        "    curr_targ = targets[i].lower()\n",
        "\n",
        "    if curr_pred == curr_targ:\n",
        "      em += 1\n",
        "\n",
        "  return round(em / len(predictions), 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGsKIU7EqkB9"
      },
      "source": [
        "Define Training Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1VC1-f7FZ6H"
      },
      "outputs": [],
      "source": [
        "# Train loop for one epoch\n",
        "def train_one_epoch(model,epoch_ind,loss_fn,optimizer):\n",
        "    # current_loss tracks loss of this batch\n",
        "    current_loss = 0.0\n",
        "    # total_loss tracks loss of this epoch\n",
        "    total_loss = 0.0 \n",
        "    for i,data in enumerate(lstm_train_dataloader):\n",
        "        inputs = data[0]\n",
        "        targets = data[1]\n",
        "\n",
        "        # Enable use of GPU\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        inputs = inputs.to(device)\n",
        "        targets = targets.to(device)\n",
        "        targets = targets.to(torch.float32)\n",
        "\n",
        "        # Get model's predictions\n",
        "        predictions = model(inputs)\n",
        "\n",
        "        # Compute loss, gradients and update model\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = loss_fn(predictions,targets)\n",
        "        loss.backward(retain_graph=True)\n",
        "        optimizer.step()\n",
        "\n",
        "        # Record loss of every batch\n",
        "        current_loss = loss.item()\n",
        "\n",
        "        total_loss += current_loss\n",
        "    return total_loss / (i+1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RImIn1HtFfox"
      },
      "outputs": [],
      "source": [
        "# Training and validation loop\n",
        "def train_model(model,num_epochs, loss_fn, optimizer):\n",
        "    timestamp = datetime.now().strftime('%m%d_%H%M')\n",
        "    epoch_ind = 0\n",
        "    # best_vloss is set to arbitrarily high number \n",
        "    best_vloss = 1000000.0\n",
        "\n",
        "    # Write training loss and validation loss of each epoch to csv file\n",
        "    out = open('./lstm_losses.csv', 'w')\n",
        "    writer = csv.writer(out)\n",
        "    writer.writerow([\"Epoch\",\"Training Loss\",\"Validation Loss\"])\n",
        "\n",
        "    # For each epoch, train model and compare against validation loss\n",
        "    for i in range(num_epochs):\n",
        "        model.train(True)\n",
        "        avg_loss = train_one_epoch(model,i+1,loss_fn, optimizer)\n",
        "        # Setting model.train(False) freezes the weights for computing valid loss\n",
        "        model.train(False)\n",
        "\n",
        "        # Compute validation loss, F1 score, and EM\n",
        "        avg_vloss = 0.0\n",
        "        total_vloss = 0.0\n",
        "        v_avg_f1 = 0\n",
        "        v_avg_em = 0\n",
        "\n",
        "        for j,data in enumerate(lstm_valid_dataloader):\n",
        "            inputs,targets = data\n",
        "\n",
        "            # Enable use of GPU\n",
        "            device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "            inputs = inputs.to(device)\n",
        "            targets = targets.to(device)\n",
        "            #targets = targets.to(torch.float32)\n",
        "\n",
        "            # Get model's predictions\n",
        "            outputs = model(inputs)\n",
        "          \n",
        "            # Compute validation loss\n",
        "            vloss = loss_fn(outputs,targets)\n",
        "            total_vloss += vloss.item()\n",
        "\n",
        "            # Separate model output into start and end indices predictions\n",
        "            splitted = torch.tensor_split(outputs, 2, dim=1)\n",
        "            start_preds = splitted[0]\n",
        "            end_preds = splitted[1]\n",
        "            \n",
        "            # Convert predicted indices to answer spans\n",
        "            predictions = []\n",
        "            for i in range(len(start_preds)):\n",
        "              answer_start = int(start_preds[i])\n",
        "              answer_end = int(end_preds[i])\n",
        "              answer = context_valid[i][answer_start:answer_end]\n",
        "              predictions.append(answer)\n",
        "        \n",
        "            # Convert target indices to answer spans\n",
        "            targets = []\n",
        "            for i in range(len(start_preds)):\n",
        "              answer_start = valid_start[i]\n",
        "              answer_end = valid_end[i]\n",
        "              answer = context_valid[i][answer_start:answer_end]\n",
        "              targets.append(answer)\n",
        "\n",
        "            # Calculate f1 score\n",
        "            f1 = compute_f1(predictions, targets)\n",
        "            v_avg_f1 += f1\n",
        "            print(\"F1 for this batch: {}\".format(f1))\n",
        "\n",
        "            # Calculate exact match\n",
        "            em = compute_em(predictions, targets)\n",
        "            print(\"EM for this batch: {}\".format(em))\n",
        "            v_avg_em += em\n",
        "\n",
        "        #Print relevant statistics about this epoch of training\n",
        "        print(\"epoch: {}\".format(epoch_ind + 1))\n",
        "\n",
        "        v_avg_f1 = v_avg_f1 / (j + 1)\n",
        "        v_avg_em = v_avg_em / (j + 1)\n",
        "        print('Average F1 score: {}'.format(v_avg_f1))\n",
        "        print('Average EM: {}'.format(v_avg_em))\n",
        "            \n",
        "        avg_vloss = total_vloss/(j+1)\n",
        "        print('LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
        "        \n",
        "        # Recording average training and validation loss in csv file\n",
        "        writer.writerow([epoch_ind + 1,avg_loss,avg_vloss])\n",
        "\n",
        "        # Save the model's state from the epoch with lowest validation loss\n",
        "        if (avg_vloss < best_vloss):\n",
        "            best_vloss = avg_vloss\n",
        "            model_path = 'model_{}_{}_{}'.format(type(model).__name__, epoch_ind, timestamp)\n",
        "            torch.save(model.state_dict(), model_path)\n",
        "        epoch_ind += 1\n",
        "    out.close\n",
        "    return model_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_IaaWARqt26"
      },
      "source": [
        "Define Testing Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EtiqQJTWFcxK"
      },
      "outputs": [],
      "source": [
        "# Testing loop\n",
        "def test_model(model):\n",
        "    t_avg_f1 = 0\n",
        "    t_avg_em = 0\n",
        "    with torch.no_grad():\n",
        "      for i,data in enumerate(lstm_test_dataloader):\n",
        "          inputs,targets = data\n",
        "\n",
        "          # Enable use of GPU\n",
        "          device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "          inputs = inputs.to(device)\n",
        "\n",
        "          # Get model output\n",
        "          outputs = model(inputs)\n",
        "\n",
        "          # Separate model output into start and end indices predictions\n",
        "          splitted = torch.tensor_split(outputs, 2, dim=1)\n",
        "          start_preds = splitted[0]\n",
        "          end_preds = splitted[1]\n",
        "\n",
        "          # Convert predicted indices to answer spans\n",
        "          predictions = []\n",
        "          for i in range(len(start_preds)):\n",
        "            answer_start = int(start_preds[i])\n",
        "            answer_end = int(end_preds[i])\n",
        "            answer = context_tst[i][answer_start:answer_end]\n",
        "            predictions.append(answer)\n",
        "          \n",
        "          # Convert target indices to answer spans\n",
        "          targets = []\n",
        "          for i in range(len(start_preds)):\n",
        "            answer_start = test_start[i]\n",
        "            answer_end = test_end[i]\n",
        "            answer = context_tst[i][answer_start:answer_end]\n",
        "            targets.append(answer)\n",
        "\n",
        "          # Calculate f1\n",
        "          f1 = compute_f1(predictions, targets)\n",
        "          t_avg_f1 += f1\n",
        "          print(\"F1 for this batch: {}\".format(f1))\n",
        "\n",
        "          # Calculate exact match\n",
        "          em = compute_em(predictions, targets)\n",
        "          print(\"EM for this batch: {}\".format(em))\n",
        "          t_avg_em += em\n",
        "\n",
        "    # Print relevant statistics\n",
        "    t_avg_f1 = t_avg_f1 / (i + 1)\n",
        "    t_avg_em = t_avg_em / (i + 1)\n",
        "    print('Average F1 score: {}'.format(t_avg_f1))\n",
        "    print('Average EM: {}'.format(t_avg_em))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5RJXZTbEGm3"
      },
      "source": [
        "Train and Evaluate the LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEz-gBdVMpD_"
      },
      "outputs": [],
      "source": [
        "# Init training parameters\n",
        "model = BiLSTM(vocab_size+1,200,200,max_len,2)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "\n",
        "loss_fn = nn.MSELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "num_epochs = 25"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVrHrecgKzXQ"
      },
      "outputs": [],
      "source": [
        "# Train model\n",
        "model_path = train_model(model,num_epochs,loss_fn,optimizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TsTMQtTgu0Su"
      },
      "outputs": [],
      "source": [
        "# Load trained model\n",
        "model.load_state_dict(torch.load(model_path))\n",
        "model.eval()\n",
        "\n",
        "# Evaluate model\n",
        "test_model(model)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "pPZ_S9Yv8sKS",
        "3dnr7SxCLNvV"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}